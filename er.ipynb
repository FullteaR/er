{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "er.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FullteaOfEEIC/er/blob/master/er.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SmMrQUF1obdk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#seq2seq\n",
        "!git clone https://www.github.com/farizrahman4u/recurrentshop.git\n",
        "%cd recurrentshop\n",
        "!python setup.py install\n",
        "!pip install git+https://github.com/farizrahman4u/seq2seq.git\n",
        "from seq2seq.models import SimpleSeq2Seq,AttentionSeq2Seq\n",
        "import keras\n",
        "\n",
        "#mecab\n",
        "!apt install aptitude\n",
        "!apt-get install swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "import MeCab\n",
        "%cd\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n\n",
        "\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "\n",
        "\n",
        "#Google Drive\n",
        "#Notes. Colaboratory may requires you a permission to access Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "%pwd\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import math\n",
        "import re\n",
        "%matplotlib inline\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYDWqi2w8ieW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"\n",
        "m=MeCab.Tagger(\"-d  /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd  -Owakati\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FufjPsKDhG6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_wv = word2vec.Word2Vec.load(\"./model/wiki.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "doH6gFAQYA9u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ASCII_ZENKAKU_CHARS = (\n",
        "    u'ａ', u'ｂ', u'ｃ', u'ｄ', u'ｅ', u'ｆ', u'ｇ', u'ｈ', u'ｉ', u'ｊ', u'ｋ',\n",
        "    u'ｌ', u'ｍ', u'ｎ', u'ｏ', u'ｐ', u'ｑ', u'ｒ', u'ｓ', u'ｔ', u'ｕ', u'ｖ',\n",
        "    u'ｗ', u'ｘ', u'ｙ', u'ｚ',\n",
        "    u'Ａ', u'Ｂ', u'Ｃ', u'Ｄ', u'Ｅ', u'Ｆ', u'Ｇ', u'Ｈ', u'Ｉ', u'Ｊ', u'Ｋ',\n",
        "    u'Ｌ', u'Ｍ', u'Ｎ', u'Ｏ', u'Ｐ', u'Ｑ', u'Ｒ', u'Ｓ', u'Ｔ', u'Ｕ', u'Ｖ',\n",
        "    u'Ｗ', u'Ｘ', u'Ｙ', u'Ｚ',\n",
        "    u'！', u'”', u'＃', u'＄', u'％', u'＆', u'’', u'（', u'）', u'＊', u'＋',\n",
        "    u'，', u'－', u'．', u'／', u'：', u'；', u'＜', u'＝', u'＞', u'？', u'＠',\n",
        "    u'［', u'￥', u'］', u'＾', u'＿', u'‘', u'｛', u'｜', u'｝', u'～', u'　'\n",
        ")\n",
        "\n",
        "ASCII_HANKAKU_CHARS = (\n",
        "    u'a', u'b', u'c', u'd', u'e', u'f', u'g', u'h', u'i', u'j', u'k',\n",
        "    u'l', u'm', u'n', u'o', u'p', u'q', u'r', u's', u't', u'u', u'v',\n",
        "    u'w', u'x', u'y', u'z',\n",
        "    u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H', u'I', u'J', u'K',\n",
        "    u'L', u'M', u'N', u'O', u'P', u'Q', u'R', u'S', u'T', u'U', u'V',\n",
        "    u'W', u'X', u'Y', u'Z',\n",
        "    u'!', u'\"', u'#', u'$', u'%', u'&', u'\\'', u'(', u')', u'*', u'+',\n",
        "    u',', u'-', u'.', u'/', u':', u';', u'<', u'=', u'>', u'?', u'@',\n",
        "    u'[', u\"¥\", u']', u'^', u'_', u'`', u'{', u'|', u'}', u'~', u' '\n",
        ")\n",
        "\n",
        "KANA_ZENKAKU_CHARS = (\n",
        "    u'ア', u'イ', u'ウ', u'エ', u'オ', u'カ', u'キ', u'ク', u'ケ', u'コ',\n",
        "    u'サ', u'シ', u'ス', u'セ', u'ソ', u'タ', u'チ', u'ツ', u'テ', u'ト',\n",
        "    u'ナ', u'ニ', u'ヌ', u'ネ', u'ノ', u'ハ', u'ヒ', u'フ', u'ヘ', u'ホ',\n",
        "    u'マ', u'ミ', u'ム', u'メ', u'モ', u'ヤ', u'ユ', u'ヨ',\n",
        "    u'ラ', u'リ', u'ル', u'レ', u'ロ', u'ワ', u'ヲ', u'ン',\n",
        "    u'ァ', u'ィ', u'ゥ', u'ェ', u'ォ', u'ッ', u'ャ', u'ュ', u'ョ',\n",
        "    u'。', u'、', u'・', u'゛', u'゜', u'「', u'」', u'ー'\n",
        ")\n",
        "\n",
        "KANA_HANKAKU_CHARS = (\n",
        "    u'ｱ', u'ｲ', u'ｳ', u'ｴ', u'ｵ', u'ｶ', u'ｷ', u'ｸ', u'ｹ', u'ｺ',\n",
        "    u'ｻ', u'ｼ', u'ｽ', u'ｾ', u'ｿ', u'ﾀ', u'ﾁ', u'ﾂ', u'ﾃ', u'ﾄ',\n",
        "    u'ﾅ', u'ﾆ', u'ﾇ', u'ﾈ', u'ﾉ', u'ﾊ', u'ﾋ', u'ﾌ', u'ﾍ', u'ﾎ',\n",
        "    u'ﾏ', u'ﾐ', u'ﾑ', u'ﾒ', u'ﾓ', u'ﾔ', u'ﾕ', u'ﾖ',\n",
        "    u'ﾗ', u'ﾘ', u'ﾙ', u'ﾚ', u'ﾛ', u'ﾜ', u'ｦ', u'ﾝ',\n",
        "    u'ｧ', u'ｨ', u'ｩ', u'ｪ', u'ｫ', u'ｯ', u'ｬ', u'ｭ', u'ｮ',\n",
        "    u'｡', u'､', u'･', u'ﾞ', u'ﾟ', u'｢', u'｣', u'ｰ'\n",
        ")\n",
        "\n",
        "DIGIT_ZENKAKU_CHARS = (\n",
        "    u'０', u'１', u'２', u'３', u'４', u'５', u'６', u'７', u'８', u'９'\n",
        ")\n",
        "\n",
        "DIGIT_HANKAKU_CHARS = (\n",
        "    u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9'\n",
        ")\n",
        "\n",
        "KANA_TEN_MAP = (\n",
        "    (u'ガ', u'ｶ'), (u'ギ', u'ｷ'), (u'グ', u'ｸ'), (u'ゲ', u'ｹ'), (u'ゴ', u'ｺ'),\n",
        "    (u'ザ', u'ｻ'), (u'ジ', u'ｼ'), (u'ズ', u'ｽ'), (u'ゼ', u'ｾ'), (u'ゾ', u'ｿ'),\n",
        "    (u'ダ', u'ﾀ'), (u'ヂ', u'ﾁ'), (u'ヅ', u'ﾂ'), (u'デ', u'ﾃ'), (u'ド', u'ﾄ'),\n",
        "    (u'バ', u'ﾊ'), (u'ビ', u'ﾋ'), (u'ブ', u'ﾌ'), (u'ベ', u'ﾍ'), (u'ボ', u'ﾎ'),\n",
        "    (u'ヴ', u'ｳ')\n",
        ")\n",
        "\n",
        "KANA_MARU_MAP = (\n",
        "    (u'パ', u'ﾊ'), (u'ピ', u'ﾋ'), (u'プ', u'ﾌ'), (u'ペ', u'ﾍ'), (u'ポ', u'ﾎ')\n",
        ")\n",
        "\n",
        "\n",
        "ascii_zh_table = {}\n",
        "ascii_hz_table = {}\n",
        "kana_zh_table = {}\n",
        "kana_hz_table = {}\n",
        "digit_zh_table = {}\n",
        "digit_hz_table = {}\n",
        "\n",
        "for (az, ah) in zip(ASCII_ZENKAKU_CHARS, ASCII_HANKAKU_CHARS):\n",
        "    ascii_zh_table[az] = ah\n",
        "    ascii_hz_table[ah] = az\n",
        "\n",
        "for (kz, kh) in zip(KANA_ZENKAKU_CHARS, KANA_HANKAKU_CHARS):\n",
        "    kana_zh_table[kz] = kh\n",
        "    kana_hz_table[kh] = kz\n",
        "\n",
        "for (dz, dh) in zip(DIGIT_ZENKAKU_CHARS, DIGIT_HANKAKU_CHARS):\n",
        "    digit_zh_table[dz] = dh\n",
        "    digit_hz_table[dh] = dz\n",
        "\n",
        "kana_ten_zh_table = {}\n",
        "kana_ten_hz_table = {}\n",
        "kana_maru_zh_table = {}\n",
        "kana_maru_hz_table = {}\n",
        "for (ktz, kth) in KANA_TEN_MAP:\n",
        "    kana_ten_zh_table[ktz] = kth\n",
        "    kana_ten_hz_table[kth] = ktz\n",
        "\n",
        "for (kmz, kmh) in KANA_MARU_MAP:\n",
        "    kana_maru_zh_table[kmz] = kmh\n",
        "    kana_maru_hz_table[kmh] = kmz\n",
        "\n",
        "del ASCII_ZENKAKU_CHARS, ASCII_HANKAKU_CHARS, \\\n",
        "    KANA_ZENKAKU_CHARS, KANA_HANKAKU_CHARS,\\\n",
        "    DIGIT_ZENKAKU_CHARS, DIGIT_HANKAKU_CHARS,\\\n",
        "    KANA_TEN_MAP, KANA_MARU_MAP\n",
        "\n",
        "kakko_zh_table = {\n",
        "    u'｟': u'⦅', u'｠': u'⦆',\n",
        "    u'『': u'｢', u'』': u'｣',\n",
        "    u'〚': u'⟦', u'〛': u'⟧',\n",
        "    u'〔': u'❲', u'〕': u'❳',\n",
        "    u'〘': u'⟬', u'〙': u'⟭',\n",
        "    u'《': u'⟪', u'》': u'⟫',\n",
        "    u'【': u'(', u'】': u')',\n",
        "    u'〖': u'(', u'〗': u')'\n",
        "}\n",
        "kakko_hz_table = {}\n",
        "for k, v in kakko_zh_table.items():\n",
        "    kakko_hz_table[v] = k\n",
        "\n",
        "\n",
        "def zen2han(text=\"\", ascii_=True, digit=True, kana=True, kakko=True, ignore=()):\n",
        "    result = []\n",
        "    for c in text:\n",
        "        if c in ignore:\n",
        "            result.append(c)\n",
        "        elif ascii_ and (c in ascii_zh_table):\n",
        "            result.append(ascii_zh_table[c])\n",
        "        elif digit and (c in digit_zh_table):\n",
        "            result.append(digit_zh_table[c])\n",
        "        elif kana and (c in kana_zh_table):\n",
        "            result.append(kana_zh_table[c])\n",
        "        elif kana and (c in kana_ten_zh_table):\n",
        "            result.append(kana_ten_zh_table[c] + u'ﾞ')\n",
        "        elif kana and (c in kana_maru_zh_table):\n",
        "            result.append(kana_maru_zh_table[c] + u'ﾟ')\n",
        "        elif kakko and (c in kakko_zh_table):\n",
        "            result.append(kakko_zh_table[c])\n",
        "        else:\n",
        "            result.append(c)\n",
        "    return \"\".join(result)\n",
        "\n",
        "\n",
        "def han2zen(text, ascii_=True, digit=True, kana=True, kakko=True, ignore=()):\n",
        "    result = []\n",
        "    for i, c in enumerate(text):\n",
        "        if c == u'ﾞ' or c == u'ﾟ':\n",
        "            continue\n",
        "        elif c in ignore:\n",
        "            result.append(c)\n",
        "        elif ascii_ and (c in ascii_hz_table):\n",
        "            result.append(ascii_hz_table[c])\n",
        "        elif digit and (c in digit_hz_table):\n",
        "            result.append(digit_hz_table[c])\n",
        "        elif kana and (c in kana_ten_hz_table) and (text[i + 1] == u'ﾞ'):\n",
        "            result.append(kana_ten_hz_table[c])\n",
        "        elif kana and (c in kana_maru_hz_table) and (text[i + 1] == u'ﾟ'):\n",
        "            result.append(kana_maru_hz_table[c])\n",
        "        elif kana and (c in kana_hz_table):\n",
        "            result.append(kana_hz_table[c])\n",
        "        elif kakko and (c in kakko_hz_table):\n",
        "            result.append(kakko_hz_table[c])\n",
        "        else:\n",
        "            result.append(c)\n",
        "    return \"\".join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OGQbG5vM9qoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = re.compile(r\"http(s)?://([\\w\\-]+\\.)+[\\w\\-]+(\\/[\\w\\- ./?%&=]*)?\")\n",
        "def line2vec(text,length=20):\n",
        "  #記号類\n",
        "  text = re.sub(\"[！!]+\",\"!\",text)\n",
        "  text = re.sub(\"[？?]+\",\"?\",text)\n",
        "  text = zen2han(text, kana=False)\n",
        "  text = han2zen(text, ascii_=False, digit=False, kakko=False)\n",
        "  text = re.sub(\"\\d+\",\"0\",text)\n",
        "  text = text.lower()\n",
        "  text = re.sub(\"w+\",\"w\",text)\n",
        "  text = url.sub(\"\",text)\n",
        "  text = text.replace(\"[ファイル]\",\"\")\n",
        "  \n",
        "  #OLK特有の固有名詞、他\n",
        "  text = text.replace(\"olk\",\"オリエンテーリング サークル\")\n",
        "  text = text.replace(\"メンブレ\",\"メンタル ブレイク\")\n",
        "  text = text.replace(\"ブリケ\",\"ストレージ\")\n",
        "  text = re.sub(\"オリエンテーリング|オリエン\",\"オリエンテーリング\",text)\n",
        "  text = text.replace(\"スクショ\",\"スクリーンショット\")\n",
        "  text = text.replace(\"追いコン\",\"追い出し コンパ\")\n",
        "  \n",
        "  #人名#この置換処理は個人情報保護の観点から記載していません。\n",
        "\n",
        "  \n",
        "  parsed = m.parse(text).strip().split(\" \")\n",
        "  \n",
        "  vector = [model_wv[p] for p in parsed if p in model_wv]\n",
        "  if length:\n",
        "    if length<len(vector):\n",
        "      vector = vector[len(vector)-length::]\n",
        "    else:\n",
        "      for _ in range(length-len(vector)):\n",
        "        vector.append(model_wv[\"。\"])\n",
        "  return vector\n",
        "  \n",
        "def vec2line(vector):\n",
        "  text = \"\"\n",
        "  for v in vector:\n",
        "     text += model_wv.most_similar( [ v ], [], 1)[0][0]\n",
        "  return text\n",
        "  \n",
        "def inflation(vector):\n",
        "  vector += np.random.randn(*vector.shape)*0.1\n",
        "  return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AQ7kkzRYI82X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q=[]\n",
        "a=[]\n",
        "with open(\"talk.txt\") as fp:\n",
        "  for line in fp:\n",
        "    name,text=line.split(\"\\t\")\n",
        "    if name==\"\":#個人情報保護の観点から削除しています\n",
        "      a.append(line2vec(text,length=20))\n",
        "    else:\n",
        "      q.append(line2vec(text,length=20)[::-1])\n",
        "q=np.asarray(q)\n",
        "a=np.asarray(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yRDFUOV8vs5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q=q.reshape(-1,20,200)\n",
        "a=a.reshape(-1,20,200)\n",
        "q_origin=copy.deepcopy(q)\n",
        "a_origin=copy.deepcopy(a)\n",
        "\n",
        "for i in range(10):\n",
        "  q=np.vstack((q,inflation(q_origin)))\n",
        "  a=np.vstack((a,inflation(a_origin)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7Y2t5NuHQcQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = AttentionSeq2Seq(input_dim=200,input_length=20, output_length=20, output_dim=200)\n",
        "\n",
        "# 学習の設定\n",
        "model.compile(loss='mse', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZAGLh2B6NGq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(q, a, epochs=150, batch_size=32)\n",
        "model.save(\"er_line_without_inflation.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40ykvrIXlVHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(\"er.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M3iSp6q6ui5c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def talk(text):\n",
        "  vector=np.asarray(line2vec(text,length=20)[::-1])\n",
        "  vector=vector.reshape(1,20,200)\n",
        "  predict=model.predict(vector,batch_size=132)\n",
        "  return vec2line(predict.reshape(20,200))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KB7xVg0M3Kfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "talk(\"元気にしてる?\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}